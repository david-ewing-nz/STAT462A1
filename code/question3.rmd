

# ---------------------------

## Description

```{r possum-map, echo=FALSE, out.width="70%", fig.align='center'}
  # Load data-set and visualise
  knitr::include_graphics("../images/possum_age_plot.png")

```
## Discription




## Load Data

```{r load-possums, include=F, eval=F}

# Load and preprocess data-set
# Performed in framework file as agreed 20250313
zip_path <- "../data/datasets.zip"
file.exists(zip_path)
possums <- read.csv(unz(zip_path, "possums.csv"))

```

## EDA - Data Cleaning and  Initial Analysis

```{r possums-EDA-prep, include=F , eval=T}

set.seed(82171165) 
pskim   <- skim(possums) |> select(skim_variable, n_missing)
print(pskim)                           
head(possums)    
summary(possums)

# possums_0 is the working copy of possums
# we will:
#  drop case - as this is just a numeric row index
#  drop Pop - it is redundant as we have site. 
#  relocate age to the last column for ease of analysis 
#  impute missing data 
#  capture numeric predictors before altering sex or site
#  mutate sex to numberic where female = 1
#  convert site to a factor
#
possums_0 <- subset(possums, select = -c(case, Pop))           # drop case and Pop

# if impute is to be considered,  predictive mean matching finds the 
# closest observed values in the dataset and randomly selects one to 
# impute the missing value.
#
possums_0 <- mice(possums_0, m=1, method="pmm") |> complete() # impute predictive mean matching

possums_0 <- possums_0 |> relocate(age, .after = last_col())   # age last 
names(possums_0)                                               # just checking
pred_names_0  <- possums_0 |> 
                    select(where(is.numeric)) |>  
                    select(-age) |> names() 
possums_0$sex <- as.numeric(possums_0$sex == "f")  # true if female

# site to be converted to a factor or should    
# the factors be separated into separate columns? 
#
#possums_0$site <- as.factor(possums_0$site)
 possums_0      <- dummy_cols(possums_0, select_columns = "site", 
                              remove_first_dummy = T,          # all zeros = site1
                              remove_selected_columns = T)     # site is dropped

 
```

```{r visualise-age, include=F}

head(possums_0)
summary(possums_0)

# visualise age 
#
ggplot(possums_0, aes(x = age)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  ggtitle("Possum Ages Distribution (imputed and without outliers)") +
  theme_minimal()
```

```{r possum-data-splitting}

# data-splitting is performed before cleaning predictors 
# to simulate real-world events. 
# 

# possums_1 is the training possum data (with stratification for age)
#
pTrainIdx <- createDataPartition(possums_0$age, p = 0.8, list = FALSE)
possums_1 <- possums_0[pTrainIdx, ]

# pTestDf     is the test possum data       (with stratification)
# pValidateDf is the validation possum data (with stratification) 
#
pTempDf     <- possums_0[-pTrainIdx, ]  #not train
pTestIdx    <- createDataPartition(pTempDf$age, p = 0.5, list = FALSE) 
pTestDf     <- pTempDf[pTestIdx, ]   
pValidateDf <- pTempDf[-pTestIdx, ]

cat("\npossums_1:   ", nrow(possums_1), "\npTestDf:     ", nrow(pTestDf), "\nVpValidateDf: ", nrow(pValidateDf),"\n")

```
```{r models-evaluation}

# Define all possible predictor combinations
#
predictors <- names(possums_1)[!names(possums_1) %in% c("age", "sex", "site")]  # Exclude categorical
best_model <- regsubsets(age ~ ., data = possums_1, nvmax = length(predictors))

# Evaluate models on validation set
model_summary <- summary(best_model)
best_model_idx <- which.min(model_summary$bic)  # Select best model based on BIC
best_predictors <- model_summary$which[best_model_idx, ]
best_formula <- as.formula(paste("age ~", paste(names(best_predictors)[-1], collapse = " + ")))

# Fit the best model
final_model <- lm(best_formula, data = possums_1)

# Evaluate on validation set
preds <- predict(final_model, newdata = pValidateDf)
mse_validation <- mean((pValidateDf$age - preds)^2)

```


\newpage
## Feature Selection and Model Training



## Model Evaluation

```{r}
# Residuals vs. Leverage plot shows potential high-leverage points, which may be influencing the model too much

# Compute evaluation metrics
```

## Further Exploration

```{r}
# Additional analysis or research questions 
#[204]
```
